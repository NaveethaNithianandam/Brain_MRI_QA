# -*- coding: utf-8 -*-
"""MR-ART(2D+3D_27avg)+ABIDEtest.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UnztSPB4XmNjG-25y3lr4pLFAjFTYK90
"""

!pip install nibabel
from google.colab import drive
drive.mount('/content/drive')

"""Vector formation for 2D method."""

# Step 1: Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Step 2: Import necessary libraries
import os
import pandas as pd
import nibabel as nib
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Step 3: Load the scores from the tsv file
scores_path = '/content/drive/MyDrive/dataset_2_masked/derivatives/scores.tsv'
scores_df = pd.read_csv(scores_path, sep='\t')

# Filter scores for subjects with a score of 1, 2, or 3
filtered_scores_df = scores_df[scores_df.iloc[:, 1].isin([1, 2, 3])]
filtered_subjects = filtered_scores_df.iloc[:, 0].apply(lambda x: x.split('_')[0]).unique()

# Function to get the score for a given subject and acquisition
def get_score(subject_id, acquisition):
    query = f'{subject_id}_{acquisition}_T1w'
    score_row = scores_df[scores_df.iloc[:, 0] == query]
    if not score_row.empty:
        return score_row.iloc[0, 1]
    else:
        return "Score not found"

# Function to normalize the intensity of the image slice using percentile normalization
def normalize_intensity(slice_data, lower_percentile=1, upper_percentile=99):
    flattened_data = slice_data.flatten()
    lower_value = np.percentile(flattened_data, lower_percentile)
    upper_value = np.percentile(flattened_data, upper_percentile)
    normalized_data = np.clip(slice_data, lower_value, upper_value)
    normalized_data = (normalized_data - lower_value) / (upper_value - lower_value)
    return normalized_data

# Function to load the specified slice of the MRI scan
def load_slice(subject_id, acquisition, slice_number, orientation='axial'):
    base_path = '/content/drive/MyDrive/dataset_2_masked'
    brain_path = os.path.join(base_path, subject_id, f'{subject_id}_{acquisition}_masked_T1w.nii.gz')

    if os.path.exists(brain_path):
        img = nib.load(brain_path)
        data = img.get_fdata()

        # Adjust slice selection based on orientation
        if orientation == 'axial':
            slice_data = data[:, :, slice_number]
        elif orientation == 'coronal':
            slice_data = data[:, slice_number, :]
        elif orientation == 'sagittal':
            slice_data = data[slice_number, :, :]
        else:
            print(f"Invalid orientation specified: {orientation}")
            return None

        # Normalize the intensity of the slice
        normalized_slice_data = normalize_intensity(slice_data)
        return normalized_slice_data
    else:
        print(f'File for {subject_id} with acquisition {acquisition} not found.')
        return None

# Function to calculate the gradient magnitude of an image slice without smoothing
def calculate_gradient_magnitude(slice_data):
    grad_x, grad_y = np.gradient(slice_data)
    gradient_magnitude = np.sqrt(grad_x**2 + grad_y**2)
    return gradient_magnitude

# Function to calculate differences between bins
def calculate_differences(gradient_magnitude):
    flattened_data = gradient_magnitude.ravel()
    bins = 60
    hist, bin_edges = np.histogram(flattened_data, bins=bins)

    # Normalize bins 1 to 5 by the count in bin 1
    if hist[1] != 0:  # Avoid division by zero
        normalized_counts = hist[1:6] / hist[1]

        # Calculate the differences between consecutive normalized counts
        differences = [normalized_counts[i] - normalized_counts[i - 1] for i in range(1, len(normalized_counts))]

        # Calculate the final sum of these differences
        final_sum = sum(differences)
        return final_sum
    else:
        print('Bin 1 has zero pixels; normalization cannot be performed.')
        return None

# Define the orientations and slice ranges
orientation_slice_ranges = {
    'axial': range(125, 165),   # axial slices 125 to 165
    'sagittal': range(80, 120),  # sagittal slices 40 to 80
    'coronal': range(80, 120)   # coronal slices 80 to 120
}

# Initialize lists to store final sums for each orientation and score
vectors_score_1 = []
vectors_score_2 = []
vectors_score_3 = []

# Use the same 200 subjects as for the right hemisphere
score_1_subjects = filtered_scores_df[filtered_scores_df.iloc[:, 1] == 1].sample(n=100, random_state=75)
score_2_subjects = filtered_scores_df[filtered_scores_df.iloc[:, 1] == 2].sample(n=50, random_state=75)
score_3_subjects = filtered_scores_df[filtered_scores_df.iloc[:, 1] == 3].sample(n=50, random_state=75)

# Combine the selected subjects into one list
selected_subjects = pd.concat([score_1_subjects, score_2_subjects, score_3_subjects])

# Loop through the parameters and analyze the slices for the selected subjects
for _, row in selected_subjects.iterrows():
    subject_id = row.iloc[0].split('_')[0]
    acquisition = row.iloc[0].split('_')[1]
    score = row.iloc[1]

    vectors = []  # To store the vectors of size 3 for this subject

    for slice_axial, slice_sagittal, slice_coronal in zip(orientation_slice_ranges['axial'],
                                                          orientation_slice_ranges['sagittal'],
                                                          orientation_slice_ranges['coronal']):
        # Initialize final sums for all orientations
        sagittal_final_sum = None
        axial_final_sum = None
        coronal_final_sum = None

        # Load slices for each orientation
        sagittal_slice = load_slice(subject_id, acquisition, slice_sagittal, 'sagittal')
        axial_slice = load_slice(subject_id, acquisition, slice_axial, 'axial')
        coronal_slice = load_slice(subject_id, acquisition, slice_coronal, 'coronal')

        if sagittal_slice is not None:
            sagittal_gradient = calculate_gradient_magnitude(sagittal_slice)
            sagittal_final_sum = calculate_differences(sagittal_gradient)

        if axial_slice is not None:
            axial_gradient = calculate_gradient_magnitude(axial_slice)
            axial_final_sum = calculate_differences(axial_gradient)

        if coronal_slice is not None:
            coronal_gradient = calculate_gradient_magnitude(coronal_slice)
            coronal_final_sum = calculate_differences(coronal_gradient)

        # Only add the vector if all final sums are calculated
        if sagittal_final_sum is not None and axial_final_sum is not None and coronal_final_sum is not None:
            vectors.append([sagittal_final_sum, axial_final_sum, coronal_final_sum])

    # Append the vectors to the corresponding score array
    if score == 1:
        vectors_score_1.append(vectors)
    elif score == 2:
        vectors_score_2.append(vectors)
    elif score == 3:
        vectors_score_3.append(vectors)

# Function to plot 3D scatter plot
def plot_3d_scatter(vectors_score_1, vectors_score_2, vectors_score_3):
    fig = plt.figure(figsize=(10, 7))
    ax = fig.add_subplot(111, projection='3d')

    # Flatten the vectors list for each score
    vectors_score_1 = np.array(vectors_score_1).reshape(-1, 3)
    vectors_score_2 = np.array(vectors_score_2).reshape(-1, 3)
    vectors_score_3 = np.array(vectors_score_3).reshape(-1, 3)

    # Plot the vectors for score 1 (green), score 2 (red), score 3 (blue)
    ax.scatter(vectors_score_1[:, 0], vectors_score_1[:, 1], vectors_score_1[:, 2], c='g', label='Score 1', marker='o')
    ax.scatter(vectors_score_2[:, 0], vectors_score_2[:, 1], vectors_score_2[:, 2], c='r', label='Score 2', marker='^')
    ax.scatter(vectors_score_3[:, 0], vectors_score_3[:, 1], vectors_score_3[:, 2], c='b', label='Score 3', marker='s')

    # Set labels
    ax.set_xlabel('Sagittal Final Sum')
    ax.set_ylabel('Axial Final Sum')
    ax.set_zlabel('Coronal Final Sum')
    ax.set_title('3D Scatter Plot')

    ax.legend()
    plt.show()

# Call the plotting function
plot_3d_scatter(vectors_score_1, vectors_score_2, vectors_score_3)

# Print scores for all 200 volume samples
def print_scores(df):
    # Print the scores and subject information
    for _, row in df.iterrows():
        subject_id = row.iloc[0].split('_')[0]
        acquisition = row.iloc[0].split('_')[1]
        score = row.iloc[1]
        print(f"Subject: {subject_id}, Acquisition: {acquisition}, Score: {score}")

# Combine the selected subjects into one DataFrame
all_selected_subjects = pd.concat([score_1_subjects, score_2_subjects, score_3_subjects])

# Print the scores of all 200 volume samples
print_scores(all_selected_subjects)

"""MLP Training."""

from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split, KFold, cross_val_score
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from scipy.stats import mode

# Prepare the dataset
def prepare_dataset(vectors_score_1, vectors_score_2, vectors_score_3):
    X_score_1 = np.array(vectors_score_1)
    X_score_2 = np.array(vectors_score_2)
    X_score_3 = np.array(vectors_score_3)

    y_score_1 = np.ones(X_score_1.shape[0])  # Class 1
    y_score_2 = np.zeros(X_score_2.shape[0])  # Class 2 (Score 2)
    y_score_3 = np.zeros(X_score_3.shape[0])  # Class 2 (Score 3)

    X = np.concatenate([X_score_1, X_score_2, X_score_3], axis=0)
    y = np.concatenate([y_score_1, y_score_2, y_score_3], axis=0)

    return X, y

# Prepare dataset
X, y = prepare_dataset(vectors_score_1, vectors_score_2, vectors_score_3)

# Create an array of indices to keep track of the original indices
indices = np.arange(len(y))

# Split the dataset into train and test sets (60% test, 40% train)
X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(
    X, y, indices, test_size=0.5, stratify=y, random_state=45
)

# Reshape the data to process individual 3D vectors (each volume has 40 vectors of size 3)
X_train_vectors = X_train.reshape(-1, 3)
y_train_repeated = np.repeat(y_train, 40)

X_test_vectors = X_test.reshape(-1, 3)
y_test_repeated = np.repeat(y_test, 40)

# Initialize and train the MLP model
mlp = MLPClassifier(
    hidden_layer_sizes=(16, 8),
    activation='relu',
    solver='adam',
    alpha=0.001,
    batch_size='auto',
    learning_rate='adaptive',
    learning_rate_init=0.001,
    max_iter=500,
    random_state=45
)

# Perform 10-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=45)
cross_val_scores = cross_val_score(mlp, X_train_vectors, y_train_repeated, cv=kf, scoring='accuracy')
print(f'Cross-validation accuracy (per vector): {cross_val_scores.mean():.2f}')

# Fit the model on the entire training set
mlp.fit(X_train_vectors, y_train_repeated)

# Predict on the test set
y_test_pred_vectors = mlp.predict(X_test_vectors)

# Reshape predictions back to volumes (40 vectors per volume)
y_test_pred_volumes = y_test_pred_vectors.reshape(-1, 40)

# Use majority voting to determine the final class for each volume
y_test_pred_majority = mode(y_test_pred_volumes, axis=1).mode.flatten()

# Evaluate the model based on majority voting
accuracy = accuracy_score(y_test, y_test_pred_majority)
conf_matrix = confusion_matrix(y_test, y_test_pred_majority)
class_report = classification_report(y_test, y_test_pred_majority, target_names=['Class 1', 'Class 2'])

print(f'Accuracy on Test Set (majority voting): {accuracy:.2f}')
print('Classification Report:\n', class_report)

# Print subject IDs, acquisition types, scores, and predicted classes for the test set
def print_test_set_details(test_indices, y_test, y_test_pred_majority, scores_df, y_test_pred_vectors):
    for i, idx in enumerate(test_indices):
        subject_id = scores_df.iloc[idx, 0].split('_')[0]
        acquisition = scores_df.iloc[idx, 0].split('_')[1]
        score = scores_df.iloc[idx, 1]
        predicted_class = 'Class 1' if y_test_pred_majority[i] == 1 else 'Class 2'

        if y_test[i] != y_test_pred_majority[i]:  # Check if the sample is misclassified
            print(f"\nMisclassified Sample:")
            print(f"Subject: {subject_id}, Acquisition: {acquisition}, Actual Score: {score}, Predicted Class: {predicted_class}")
            # Print all 40 predicted vectors for this sample
            start_idx = i * 40
            end_idx = start_idx + 40
            print("Predictions for the 40 vectors:")
            for vector_idx in range(start_idx, end_idx):
                print(f"Vector {vector_idx - start_idx + 1}: {'Class 1' if y_test_pred_vectors[vector_idx] == 1 else 'Class 2'}")

# Combine the selected subjects into one DataFrame
all_selected_subjects = pd.concat([score_1_subjects, score_2_subjects, score_3_subjects])

# Print the scores and details of all test samples
print_test_set_details(test_indices, y_test, y_test_pred_majority, all_selected_subjects, y_test_pred_vectors)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 2', 'Class 1'], yticklabels=['Class 2', 'Class 1'])
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

"""3D method."""

import os
import random
import nibabel as nib
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve
from collections import defaultdict

# Base directory and scores file
base_dir = "/content/drive/MyDrive/dataset_2_masked"
scores_path = "/content/drive/MyDrive/dataset_2_masked/derivatives/scores.tsv"

# Load scores
df_scores = pd.read_csv(scores_path, sep="\t")

# Filter subjects with scores 1, 2, or 3
filtered_scores_df = df_scores[df_scores.iloc[:, 1].isin([1, 2, 3])]
filtered_subjects = filtered_scores_df.iloc[:, 0].apply(lambda x: x.split('_')[0]).unique()

# Define acquisitions
acquisitions = ["acq-standard", "acq-headmotion1", "acq-headmotion2"]

# Function to get the score
def get_score(subject_id, acquisition):
    query = f"{subject_id}_{acquisition}_T1w"
    score_row = df_scores[df_scores.iloc[:, 0] == query]
    return score_row.iloc[0, 1] if not score_row.empty else None

# Collect valid (subject, acquisition) pairs
instances_by_score = {1: [], 2: [], 3: []}
for subject_id in filtered_subjects:
    for acquisition in acquisitions:
        file_path = os.path.join(base_dir, subject_id, f"{subject_id}_{acquisition}_masked_T1w.nii.gz")
        score = get_score(subject_id, acquisition)
        if os.path.exists(file_path) and score in [1, 2, 3]:
            instances_by_score[score].append((subject_id, acquisition, file_path, score))

# Randomly select the required number of instances per score
random.seed(97)
selected_instances = (
    random.sample(instances_by_score[1], min(129, len(instances_by_score[1]))) +
    random.sample(instances_by_score[2], min(109, len(instances_by_score[2]))) +
    random.sample(instances_by_score[3], min(198, len(instances_by_score[3])))
)

# Define cuboid size and step size (20% overlap)
cuboid_size = (96, 128, 128)
step_size = tuple(int(s * 0.8) for s in cuboid_size)

def calculate_differences(gradient_magnitude):
    flattened_data = gradient_magnitude.ravel()
    bins = 60
    hist, bin_edges = np.histogram(flattened_data, bins=bins)

    if hist[0] != 0:
        normalized_counts = hist[:6] / hist[0]
        differences = [normalized_counts[i] - normalized_counts[i - 1] for i in range(1, len(normalized_counts))]
        return sum(differences)
    return None

# Dictionary to store final differences categorized by score
final_differences_by_score = {1: [], 2: [], 3: []}
final_differences_by_sample = []  # Track (subject_id, acquisition, score, avg_final_difference)
sample_details = []


for subject_id, acquisition, file_path, score in selected_instances:
    img = nib.load(file_path)
    data = img.get_fdata()

    final_sums = []
    z_positions = list(range(0, data.shape[0] - cuboid_size[0] + 1, step_size[0])) + [data.shape[0] - cuboid_size[0]]
    y_positions = list(range(0, data.shape[1] - cuboid_size[1] + 1, step_size[1])) + [data.shape[1] - cuboid_size[1]]
    x_positions = list(range(0, data.shape[2] - cuboid_size[2] + 1, step_size[2])) + [data.shape[2] - cuboid_size[2]]

    for z in sorted(set(z_positions)):
        for y in sorted(set(y_positions)):
            for x in sorted(set(x_positions)):
                cuboid = data[z:z+cuboid_size[0], y:y+cuboid_size[1], x:x+cuboid_size[2]]

                nonzero_mask = cuboid > 0
                cuboid_nonzero = cuboid[nonzero_mask]

                if cuboid_nonzero.size == 0:
                    continue

                p_low, p_high = np.percentile(cuboid_nonzero, [1, 99])
                cuboid_normalized = np.clip((cuboid - p_low) / (p_high - p_low), 0, 1)
                cuboid_normalized[~nonzero_mask] = 0

                grad_x, grad_y, grad_z = np.gradient(cuboid_normalized)
                grad_magnitude = np.sqrt(grad_x**2 + grad_y**2 + grad_z**2)

                grad_magnitude_nonzero = grad_magnitude[grad_magnitude > 0]
                final_difference = calculate_differences(grad_magnitude_nonzero)

                final_sums.append(final_difference)
                sample_details.append((subject_id, acquisition, score, final_difference))


    avg_final_difference = np.mean(final_sums)
    final_differences_by_score[score].append(avg_final_difference)
    final_differences_by_sample.append((subject_id, acquisition, score, avg_final_difference))

# Compute average final differences per sample
average_final_differences_by_sample = defaultdict(list)
for subject, acquisition, score, value in final_differences_by_sample:
    average_final_differences_by_sample[(subject, acquisition, score)].append(value)

# Take the mean of the 27 values per sample
average_final_differences = {key: np.mean(values) for key, values in average_final_differences_by_sample.items()}

# Prepare data for ROC curve (Class 1 vs. Class 2 & 3)
labels = np.array([1 if key[2] == 1 else 2 for key in average_final_differences.keys()])
scores = np.array(list(average_final_differences.values()))

# Compute ROC curve
fpr, tpr, thresholds = roc_curve(labels, scores, pos_label=2)

# Find the threshold with maximum Youden's index
youden_index = tpr - fpr
optimal_threshold = thresholds[np.argmax(youden_index)]

# Plot histogram of average final differences
plt.figure(figsize=(8, 8))
plt.hist([val for key, val in average_final_differences.items() if key[2] == 1], bins=30, color='green', alpha=0.5, edgecolor='black', label='Score 1')
plt.hist([val for key, val in average_final_differences.items() if key[2] == 2], bins=30, color='blue', alpha=0.5, edgecolor='black', label='Score 2')
plt.hist([val for key, val in average_final_differences.items() if key[2] == 3], bins=30, color='red', alpha=0.5, edgecolor='black', label='Score 3')
plt.axvline(optimal_threshold, color='black', linestyle='dashed', linewidth=2, label=f'Optimal Threshold = {optimal_threshold:.2f}')
plt.xlabel("Average Final Sum of Differences")
plt.ylabel("Frequency")
plt.title("Histogram of Average Final Sum of Differences by Score")
plt.legend()
plt.show()

print(f"Optimal threshold using Youden’s Index: {optimal_threshold:.2f}")

"""Final results for fusion method."""

from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from scipy.special import expit
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix


# Get MLP confidence scores (probabilities)
mlp_probabilities = mlp.predict_proba(X_test_vectors)  # shape: (N_vectors, 2)

# Average probabilities across 40 vectors per volume
mlp_volume_probs = mlp_probabilities.reshape(-1, 40, 2).mean(axis=1)  # shape: (N_samples, 2)

# Containers
actual_classes = []
fused_predictions = []
mlp_confidences = []
gradient_confidences = []
fused_confidences = []

print("\n--- Fused Model Confidence Scores ---")
print(f"Optimal Threshold: {optimal_threshold:.4f}\n")

for i, idx in enumerate(test_indices):
    row = all_selected_subjects.iloc[idx]
    subject_id = row[0].split('_')[0]
    acquisition = row[0].split('_')[1]
    score = row[1]
    key = (subject_id, acquisition, score)

    # Actual class: 1 for score=1, else 0
    actual_class = 1 if score == 1 else 0
    actual_classes.append(actual_class)

    # MLP prediction & confidence
    mlp_pred = y_test_pred_majority[i]
    mlp_confidence_class2 = mlp_volume_probs[i][0]  # prob of class 2
    mlp_confidence_class1 = mlp_volume_probs[i][1]  # prob of class 1
    mlp_confidence = mlp_confidence_class2 if mlp_pred == 0 else mlp_confidence_class1
    mlp_confidences.append(mlp_confidence)

    # Gradient-based confidence
    avg_diff = average_final_differences.get(key, None)
    if avg_diff is not None:
        distance = avg_diff - optimal_threshold
        grad_conf_class2 = expit(distance)          # if distance > 0 → more confident it's class 2
        grad_conf_class1 = expit(-distance)         # if distance < 0 → more confident it's class 1
        threshold_status = "above threshold" if distance > 0 else "below threshold"
    else:
        grad_conf_class2 = grad_conf_class1 = 0.5   # neutral if missing
        threshold_status = "N/A"

    # Fusion prediction
    if mlp_pred == 0 or (avg_diff is not None and avg_diff > optimal_threshold):
        fused_pred_class = 0
        fused_conf = (mlp_confidence_class2 + grad_conf_class2) / 2
    else:
        fused_pred_class = 1
        fused_conf = (mlp_confidence_class1 + grad_conf_class1) / 2

    fused_predictions.append(fused_pred_class)
    fused_confidences.append(fused_conf)
    gradient_confidences.append(grad_conf_class2 if fused_pred_class == 0 else grad_conf_class1)

    print(f"Subject: {subject_id}, Acquisition: {acquisition}, Actual: {'Class 1' if actual_class else 'Class 2'}, "
          f"MLP Pred: {'Class 1' if mlp_pred else 'Class 2'}, MLP Conf: {mlp_confidence:.2f}, "
          f"Avg Final Diff: {threshold_status}, Grad Conf: {gradient_confidences[-1]:.2f} → "
          f"Fused Pred: {'Class 1' if fused_pred_class else 'Class 2'}, Fused Conf: {fused_conf:.2f}")



# Evaluate fused predictions
conf_matrix = confusion_matrix(actual_classes, fused_predictions)
class_report = classification_report(actual_classes, fused_predictions, target_names=['Class 2', 'Class 1'])
accuracy = accuracy_score(actual_classes, fused_predictions)

print("\nClassification Report:")
print(class_report)
print(f"Accuracy: {accuracy:.2f}")


# Create confusion matrix
conf_matrix = confusion_matrix(actual_classes, fused_predictions)

# Plot heatmap
plt.figure(figsize=(6, 5))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='YlGnBu',
            xticklabels=['Class 2', 'Class 1'],
            yticklabels=['Class 2', 'Class 1'])
plt.title('Fused Model – Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.tight_layout()
plt.show()

"""Unseen site testing on ABIDE 50 samples."""

import os
import random
import numpy as np
import nibabel as nib
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from scipy.special import expit  # Sigmoid
from sklearn.neural_network import MLPClassifier

# Assume the following are already defined/trained
# mlp: your trained MLPClassifier
# optimal_threshold: float value used in sigmoid thresholding

base_dir = "/content/drive/MyDrive/ABIDE_testsamples"
cuboid_size = (96, 128, 128)
step_size = tuple(int(s * 0.8) for s in cuboid_size)

# Prepare paths (random selection of 25 samples from each class)
class1_full = [os.path.join(base_dir, "class1", f) for f in os.listdir(os.path.join(base_dir, "class1")) if f.endswith('.nii') or f.endswith('.nii.gz')]
class2_full = [os.path.join(base_dir, "class2", f) for f in os.listdir(os.path.join(base_dir, "class2")) if f.endswith('.nii') or f.endswith('.nii.gz')]

# Shuffle and sample
random.seed(42)  # for reproducibility
class1_paths = random.sample(class1_full, min(25, len(class1_full)))
class2_paths = random.sample(class2_full, min(25, len(class2_full)))

all_paths = [(p, 1) for p in class1_paths] + [(p, 0) for p in class2_paths]

# Orientation slice ranges
orientation_slice_ranges = {
    'axial': list(range(120, 160)),
    'sagittal': list(range(80, 120)),
    'coronal': list(range(80, 120))
}

# Utility functions
def normalize_intensity(slice_data, lower_percentile=1, upper_percentile=99):
    flattened = slice_data.flatten()
    p1 = np.percentile(flattened, lower_percentile)
    p99 = np.percentile(flattened, upper_percentile)
    if p99 == p1:
        return np.zeros_like(slice_data)
    return np.clip((slice_data - p1) / (p99 - p1), 0, 1)

def calculate_gradient_magnitude(slice_data):
    grad_x, grad_y = np.gradient(slice_data)
    return np.sqrt(grad_x**2 + grad_y**2)

def calculate_differences(gradient_magnitude):
    flat = gradient_magnitude.ravel()
    hist, _ = np.histogram(flat, bins=60)
    if len(hist) < 6 or hist[1] == 0:
        return None
    norm_counts = hist[1:6] / hist[1]
    diffs = [norm_counts[i] - norm_counts[i - 1] for i in range(1, len(norm_counts))]
    return sum(diffs)

# Lists to collect results
actual_classes = []
fused_predictions = []
fused_confidences = []

print("\n--- ABIDE Test Samples Prediction ---")

for file_path, actual_class in all_paths:
    subject_id = os.path.basename(file_path).split('_')[0]

    img = nib.load(file_path)
    data = img.get_fdata()

    # 3D Cuboid Gradient Features
    final_sums = []
    z_positions = list(range(0, data.shape[0] - cuboid_size[0] + 1, step_size[0])) + [data.shape[0] - cuboid_size[0]]
    y_positions = list(range(0, data.shape[1] - cuboid_size[1] + 1, step_size[1])) + [data.shape[1] - cuboid_size[1]]
    x_positions = list(range(0, data.shape[2] - cuboid_size[2] + 1, step_size[2])) + [data.shape[2] - cuboid_size[2]]

    for z in sorted(set(z_positions)):
        for y in sorted(set(y_positions)):
            for x in sorted(set(x_positions)):
                cuboid = data[z:z+cuboid_size[0], y:y+cuboid_size[1], x:x+cuboid_size[2]]
                nonzero_mask = cuboid > 0
                cuboid_nonzero = cuboid[nonzero_mask]
                if cuboid_nonzero.size == 0:
                    continue
                p_low, p_high = np.percentile(cuboid_nonzero, [1, 99])
                cuboid_norm = np.clip((cuboid - p_low) / (p_high - p_low), 0, 1)
                cuboid_norm[~nonzero_mask] = 0
                grad_x, grad_y, grad_z = np.gradient(cuboid_norm)
                grad_magnitude = np.sqrt(grad_x**2 + grad_y**2 + grad_z**2)
                flattened = grad_magnitude[grad_magnitude > 0]
                bins = 60
                hist, _ = np.histogram(flattened, bins=bins)
                if hist[0] == 0:
                    continue
                norm_counts = hist[:6] / hist[0]
                diff_sum = sum([norm_counts[i] - norm_counts[i - 1] for i in range(1, len(norm_counts))])
                final_sums.append(diff_sum)

    if not final_sums:
        continue

    avg_final_diff = np.mean(final_sums)
    dist_from_thresh = avg_final_diff - optimal_threshold
    grad_conf_class2 = expit(dist_from_thresh)
    grad_conf_class1 = expit(-dist_from_thresh)

    # 2D Slice Features for MLP
    sample_vectors = []
    for orientation, slice_range in orientation_slice_ranges.items():
        for slice_idx in slice_range:
            if orientation == 'axial':
                slice_data = data[:, :, slice_idx]
            elif orientation == 'sagittal':
                slice_data = data[slice_idx, :, :]
            elif orientation == 'coronal':
                slice_data = data[:, slice_idx, :]
            else:
                continue

            norm_slice = normalize_intensity(slice_data)
            grad_mag = calculate_gradient_magnitude(norm_slice)
            diff_value = calculate_differences(grad_mag)

            if diff_value is not None:
                sample_vectors.append(diff_value)

    try:
        reshaped_vectors = np.array(sample_vectors).reshape(40, 3)
    except:
        continue  # Skip subject if reshaping fails

    if reshaped_vectors.size == 0:
        continue

    vectors_np = reshaped_vectors
    mlp_preds = mlp.predict(vectors_np)
    mlp_probs = mlp.predict_proba(vectors_np)
    majority_pred = int(np.round(np.mean(mlp_preds)))
    mlp_class1_prob = np.mean(mlp_probs[:, 1])
    mlp_class2_prob = np.mean(mlp_probs[:, 0])

    # Fusion Logic
    if majority_pred == 0 or avg_final_diff > optimal_threshold:
        fused_pred = 0
        fused_conf = (mlp_class2_prob + grad_conf_class2) / 2
    else:
        fused_pred = 1
        fused_conf = (mlp_class1_prob + grad_conf_class1) / 2

    actual_classes.append(actual_class)
    fused_predictions.append(fused_pred)
    fused_confidences.append(fused_conf)

    print(f"{file_path}\n→ Predicted: {'Class 1' if fused_pred else 'Class 2'}, Confidence: {fused_conf:.3f}\n")

# Evaluation
print("\n--- Fused Model Results on Random 50 Samples ---")
conf_matrix = confusion_matrix(actual_classes, fused_predictions)
report = classification_report(actual_classes, fused_predictions, target_names=["Class 2", "Class 1"])
accuracy = accuracy_score(actual_classes, fused_predictions)

plt.figure(figsize=(6, 5))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='coolwarm',
            xticklabels=["Class 2 (Pred)", "Class 1 (Pred)"],
            yticklabels=["Class 2 (True)", "Class 1 (True)"])
plt.title('Fused Model Confusion Matrix – 25 Random Samples Each Class')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.tight_layout()
plt.show()

print(report)
print(f"Accuracy: {accuracy:.2f}")

"""Unseen site testing on ABIDE 100 samples."""

import os
import numpy as np
import nibabel as nib
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from scipy.special import expit  # Sigmoid
from sklearn.neural_network import MLPClassifier

# Assume the following are already defined/trained
# mlp: your trained MLPClassifier
# optimal_threshold: float value used in sigmoid thresholding

base_dir = "/content/drive/MyDrive/ABIDE_testsamples"
cuboid_size = (96, 128, 128)
step_size = tuple(int(s * 0.8) for s in cuboid_size)

# Prepare paths
class1_paths = [os.path.join(base_dir, "class1", f) for f in os.listdir(os.path.join(base_dir, "class1")) if f.endswith('.nii') or f.endswith('.nii.gz')]
class2_paths = [os.path.join(base_dir, "class2", f) for f in os.listdir(os.path.join(base_dir, "class2")) if f.endswith('.nii') or f.endswith('.nii.gz')]
all_paths = [(p, 1) for p in class1_paths] + [(p, 0) for p in class2_paths]

# Orientation slice ranges
orientation_slice_ranges = {
    'axial': list(range(120, 160)),    # 40 axial slices
    'sagittal': list(range(80, 120)),  # 40 sagittal slices
    'coronal': list(range(80, 120))    # 40 coronal slices
}

# Utility functions
def normalize_intensity(slice_data, lower_percentile=1, upper_percentile=99):
    flattened = slice_data.flatten()
    p1 = np.percentile(flattened, lower_percentile)
    p99 = np.percentile(flattened, upper_percentile)
    if p99 == p1:
        return np.zeros_like(slice_data)
    clipped = np.clip(slice_data, p1, p99)
    return (clipped - p1) / (p99 - p1)

def calculate_gradient_magnitude(slice_data):
    grad_x, grad_y = np.gradient(slice_data)
    return np.sqrt(grad_x**2 + grad_y**2)

def calculate_differences(gradient_magnitude):
    flat = gradient_magnitude.ravel()
    hist, _ = np.histogram(flat, bins=60)
    if len(hist) < 6 or hist[1] == 0:
        return None
    norm_counts = hist[1:6] / hist[1]
    diffs = [norm_counts[i] - norm_counts[i - 1] for i in range(1, len(norm_counts))]
    return sum(diffs)

# Lists to collect results
actual_classes = []
fused_predictions = []
fused_confidences = []

print("\n--- ABIDE Test Samples Prediction ---")

for file_path, actual_class in all_paths:
    subject_id = os.path.basename(file_path).split('_')[0]

    # Load volume
    img = nib.load(file_path)
    data = img.get_fdata()

    # --- Gradient Feature Extraction (3D Cuboid-Based) ---
    final_sums = []
    z_positions = list(range(0, data.shape[0] - cuboid_size[0] + 1, step_size[0])) + [data.shape[0] - cuboid_size[0]]
    y_positions = list(range(0, data.shape[1] - cuboid_size[1] + 1, step_size[1])) + [data.shape[1] - cuboid_size[1]]
    x_positions = list(range(0, data.shape[2] - cuboid_size[2] + 1, step_size[2])) + [data.shape[2] - cuboid_size[2]]

    for z in sorted(set(z_positions)):
        for y in sorted(set(y_positions)):
            for x in sorted(set(x_positions)):
                cuboid = data[z:z+cuboid_size[0], y:y+cuboid_size[1], x:x+cuboid_size[2]]
                nonzero_mask = cuboid > 0
                cuboid_nonzero = cuboid[nonzero_mask]
                if cuboid_nonzero.size == 0:
                    continue
                p_low, p_high = np.percentile(cuboid_nonzero, [1, 99])
                cuboid_norm = np.clip((cuboid - p_low) / (p_high - p_low), 0, 1)
                cuboid_norm[~nonzero_mask] = 0
                grad_x, grad_y, grad_z = np.gradient(cuboid_norm)
                grad_magnitude = np.sqrt(grad_x**2 + grad_y**2 + grad_z**2)
                flattened = grad_magnitude[grad_magnitude > 0]
                bins = 60
                hist, _ = np.histogram(flattened, bins=bins)
                if hist[0] == 0:
                    continue
                norm_counts = hist[:6] / hist[0]
                diff_sum = sum([norm_counts[i] - norm_counts[i - 1] for i in range(1, len(norm_counts))])
                final_sums.append(diff_sum)

    if not final_sums:
        continue

    avg_final_diff = np.mean(final_sums)
    dist_from_thresh = avg_final_diff - optimal_threshold
    grad_conf_class2 = expit(dist_from_thresh)
    grad_conf_class1 = expit(-dist_from_thresh)

    # --- MLP Prediction using 2D Orientation-Based Slices ---
    vectors = []
    sample_vectors = []
    for orientation, slice_range in orientation_slice_ranges.items():
        for slice_idx in slice_range:
            if orientation == 'axial':
                slice_data = data[:, :, slice_idx]
            elif orientation == 'sagittal':
                slice_data = data[slice_idx, :, :]
            elif orientation == 'coronal':
                slice_data = data[:, slice_idx, :]
            else:
                continue

            norm_slice = normalize_intensity(slice_data)
            grad_mag = calculate_gradient_magnitude(norm_slice)
            diff_value = calculate_differences(grad_mag)

            if diff_value is not None:
                sample_vectors.append(diff_value)

    try:
        reshaped_vectors = np.array(sample_vectors).reshape(40, 3)
        vectors.extend(reshaped_vectors)
    except:
        continue  # Skip subject if reshaping fails

    if not vectors:
        continue

    vectors_np = np.array(vectors)
    mlp_preds = mlp.predict(vectors_np)
    mlp_probs = mlp.predict_proba(vectors_np)
    majority_pred = int(np.round(np.mean(mlp_preds)))
    mlp_class1_prob = np.mean(mlp_probs[:, 1])
    mlp_class2_prob = np.mean(mlp_probs[:, 0])

    # --- Fusion ---
    if majority_pred == 0 or avg_final_diff > optimal_threshold:
        fused_pred = 0
        fused_conf = (mlp_class2_prob + grad_conf_class2) / 2
    else:
        fused_pred = 1
        fused_conf = (mlp_class1_prob + grad_conf_class1) / 2

    actual_classes.append(actual_class)
    fused_predictions.append(fused_pred)
    fused_confidences.append(fused_conf)

    print(f"{file_path}\n→ Predicted: {'Class 1' if fused_pred else 'Class 2'}, Confidence: {fused_conf:.3f}\n")

# --- Evaluation ---
print("\n--- Fused Model Results on ABIDE Test Samples ---")
conf_matrix = confusion_matrix(actual_classes, fused_predictions)
report = classification_report(actual_classes, fused_predictions, target_names=["Class 2", "Class 1"])
accuracy = accuracy_score(actual_classes, fused_predictions)

# Heatmap
plt.figure(figsize=(6, 5))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='coolwarm',
            xticklabels=["Class 2 (Pred)", "Class 1 (Pred)"],
            yticklabels=["Class 2 (True)", "Class 1 (True)"])
plt.title('Fused Model Confusion Matrix – ABIDE Test Samples')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.tight_layout()
plt.show()

# Print metrics
print(report)
print(f"Accuracy: {accuracy:.2f}")

"""Unseen site testing on ABIDE 200 samples."""

import os
import numpy as np
import nibabel as nib
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from scipy.special import expit  # Sigmoid
from sklearn.neural_network import MLPClassifier

# Assume these are already defined
# mlp: trained MLPClassifier
# optimal_threshold: sigmoid decision threshold

# -----------------------
# Dataset Setup
# -----------------------
base_dir = "/content/drive/MyDrive/Dataset_1_preprocessed"
class1_dir = os.path.join(base_dir, "Score1_fullprocessed")  # label = 1
class2_dir = os.path.join(base_dir, "Score3_fullprocessed")  # label = 0

cuboid_size = (96, 128, 128)
step_size = tuple(int(s * 0.8) for s in cuboid_size)

# Gather file paths
class1_paths = [os.path.join(class1_dir, f) for f in os.listdir(class1_dir) if f.endswith('.nii') or f.endswith('.nii.gz')]
class2_paths = [os.path.join(class2_dir, f) for f in os.listdir(class2_dir) if f.endswith('.nii') or f.endswith('.nii.gz')]
all_paths = [(p, 1) for p in class1_paths] + [(p, 0) for p in class2_paths]

# -----------------------
# Slice Ranges by Orientation
# -----------------------
orientation_slice_ranges = {
    'axial': list(range(120, 160)),
    'sagittal': list(range(80, 120)),
    'coronal': list(range(80, 120))
}

# -----------------------
# Utility Functions
# -----------------------
def normalize_intensity(slice_data, lower_percentile=1, upper_percentile=99):
    flat = slice_data.flatten()
    p1, p99 = np.percentile(flat, [lower_percentile, upper_percentile])
    if p99 == p1:
        return np.zeros_like(slice_data)
    return np.clip((slice_data - p1) / (p99 - p1), 0, 1)

def calculate_gradient_magnitude(slice_data):
    gx, gy = np.gradient(slice_data)
    return np.sqrt(gx**2 + gy**2)

def calculate_differences(grad_mag):
    flat = grad_mag.ravel()
    hist, _ = np.histogram(flat, bins=60)
    if len(hist) < 6 or hist[1] == 0:
        return None
    norm_counts = hist[1:6] / hist[1]
    diffs = [norm_counts[i] - norm_counts[i-1] for i in range(1, len(norm_counts))]
    return sum(diffs)

# -----------------------
# Evaluation Variables
# -----------------------
actual_classes = []
fused_predictions = []
fused_confidences = []

print("\n--- Dataset_1 Prediction ---")

# -----------------------
# Main Prediction Loop
# -----------------------
for file_path, actual_class in all_paths:
    subject_id = os.path.basename(file_path).split('_')[0]

    # Load volume
    img = nib.load(file_path)
    data = img.get_fdata()

    # --- 3D Cuboid-Based Feature Extraction ---
    final_sums = []
    z_pos = list(range(0, data.shape[0] - cuboid_size[0] + 1, step_size[0])) + [data.shape[0] - cuboid_size[0]]
    y_pos = list(range(0, data.shape[1] - cuboid_size[1] + 1, step_size[1])) + [data.shape[1] - cuboid_size[1]]
    x_pos = list(range(0, data.shape[2] - cuboid_size[2] + 1, step_size[2])) + [data.shape[2] - cuboid_size[2]]

    for z in sorted(set(z_pos)):
        for y in sorted(set(y_pos)):
            for x in sorted(set(x_pos)):
                cuboid = data[z:z+cuboid_size[0], y:y+cuboid_size[1], x:x+cuboid_size[2]]
                mask = cuboid > 0
                if not np.any(mask):
                    continue
                p1, p99 = np.percentile(cuboid[mask], [1, 99])
                norm = np.clip((cuboid - p1) / (p99 - p1), 0, 1)
                norm[~mask] = 0
                gx, gy, gz = np.gradient(norm)
                grad_mag = np.sqrt(gx**2 + gy**2 + gz**2)
                flat = grad_mag[grad_mag > 0]
                hist, _ = np.histogram(flat, bins=60)
                if hist[0] == 0:
                    continue
                norm_counts = hist[:6] / hist[0]
                diff = sum([norm_counts[i] - norm_counts[i-1] for i in range(1, 6)])
                final_sums.append(diff)

    if not final_sums:
        continue

    avg_final_diff = np.mean(final_sums)
    dist_from_thresh = avg_final_diff - optimal_threshold
    grad_conf_class2 = expit(dist_from_thresh)
    grad_conf_class1 = expit(-dist_from_thresh)

    # --- 2D Slice-Based MLP Feature Vectors ---
    sample_vectors = []
    for orientation, slice_range in orientation_slice_ranges.items():
        for slice_idx in slice_range:
            if orientation == 'axial':
                slice_data = data[:, :, slice_idx]
            elif orientation == 'sagittal':
                slice_data = data[slice_idx, :, :]
            elif orientation == 'coronal':
                slice_data = data[:, slice_idx, :]
            else:
                continue
            norm_slice = normalize_intensity(slice_data)
            grad_mag = calculate_gradient_magnitude(norm_slice)
            diff = calculate_differences(grad_mag)
            if diff is not None:
                sample_vectors.append(diff)

    try:
        reshaped_vectors = np.array(sample_vectors).reshape(40, 3)
    except:
        continue

    mlp_input = reshaped_vectors
    mlp_preds = mlp.predict(mlp_input)
    mlp_probs = mlp.predict_proba(mlp_input)
    majority_pred = int(np.round(np.mean(mlp_preds)))
    mlp_class1_prob = np.mean(mlp_probs[:, 1])
    mlp_class2_prob = np.mean(mlp_probs[:, 0])

    # --- Fusion ---
    if majority_pred == 0 or avg_final_diff > optimal_threshold:
        fused_pred = 0
        fused_conf = (mlp_class2_prob + grad_conf_class2) / 2
    else:
        fused_pred = 1
        fused_conf = (mlp_class1_prob + grad_conf_class1) / 2

    actual_classes.append(actual_class)
    fused_predictions.append(fused_pred)
    fused_confidences.append(fused_conf)

    print(f"{file_path}\n→ Predicted: {'Class 1' if fused_pred else 'Class 2'}, Confidence: {fused_conf:.3f}\n")

# -----------------------
# Results and Evaluation
# -----------------------
print("\n--- Fused Model Results on Dataset_1_preprocessed ---")
conf_matrix = confusion_matrix(actual_classes, fused_predictions)
report = classification_report(actual_classes, fused_predictions, target_names=["Class 2", "Class 1"])
accuracy = accuracy_score(actual_classes, fused_predictions)

plt.figure(figsize=(6, 5))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='coolwarm',
            xticklabels=["Class 2 (Pred)", "Class 1 (Pred)"],
            yticklabels=["Class 2 (True)", "Class 1 (True)"])
plt.title('Fused Model Confusion Matrix – Dataset_1')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.tight_layout()
plt.show()

print(report)
print(f"Accuracy: {accuracy:.2f}")

